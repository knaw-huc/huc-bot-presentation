Hi, my name is HucBot.
I speak behalf of the Digital Infrastructure Department of the Humanities Cluster of the Dutch Royal Academy, who have created me.
Actually, I'm just a little piece of software, made with love by the people of Digital Infrastructure.

Like the people of the Digital Infrastructure I'm interested in history, language, stories, art, thoughts, data, technology and how all these interesting but complex things come together. Let me tell you a little bit about us.

Our department comes from three humanities institutes from the Netherlands.  We've already build a lots of tools for the humanities. The most of those tools are little Islands. Our primairy aim is to connect these tools. We want to let researchers select data from the archives and run these through chains of connected language and image software. Extract information and enrich original sources. The results are then returned to scientists for further human analysis.

Building these connections is what turns software into infrastructure. The magic word for that is 'open standards'.  This will allow us to combine datasets from entirely different fields. Such as historical sources with archeology, cultural heritage, governments, and corporations. And it also opens up humanities research for further use by the media, policy makers, journalists, teachers and many others.

Looking at archive and research infrastructure. These must be clearly separated.

Researchers not just desire to work with our own collections. They need to be able to analyze data from all over the Netherlands, from Europe, and actually from across the globe. Archive infrastructure needs to be sustainable, and expose only carefully curated data. Research infrastructure on the other hand is volatile. It is dynamic and subject to change. It must be able to handle data that is not cleaned, or carefully selected. It needs to be open for experimentation.

Lets take a closer look at Research Infrastructure. We distinguish three data streams in Research Infrastructure. The first handles the digital representation of reality. It contains: Images, Audio and Video. Image software, for example, can extract text, detect faces, analyze ink color, or identify typewriters.

The second stream continues where the first stops and handles transcribed reality. It works not only with manually transcribed sources but more and more with automatically generated texts. Through optical character recognition, hand-written text analyses, or speech-to-text audio processing.With a transcription of varying quality available we can start extracting linguistic elements, such as nouns, verbs, and sentence structure, as well as detect names of people, places, objects and dates.

Detecting that a combination of letters is a noun, and likely the name of a person, is only half the job. To make sense we need to know who the name actually belongs to. To do that we come to the third and final data stream: structured data. This is the semantic representation of reality. Large graphs of identified people, with birth date and location, and many other characteristics. We add these records to locations, objects, events, that were collected over centuries of research. Combined they form knowledge graphs and explain how people are connected, where they worked, lived, and traveled. A process called: disambiguation, identifies the name that was recognized in a transcription. It links the occurrence in a text, to the known person in the knowledge graph, and makes the entire context available to the researcher.

Through this process we are capable of working with 17th century shipping records, identify the sailors on board, and follow their careers. We can follow the voyages of individual ships, goods, texts, books, and even ideas, through time and space.

TRIADO is one of those projects in which a lot of technology comes together. The project is funded by the Academy’s research fund and aims to automate access to the archives of the Dutch tribunals prosecuting crimes committed in the country during World War Two. Together with the National Archives and the Dutch Institute for War, Holocaust and Genocide Studies we are digitizing thirteen meters of dossiers as a pilot sample. The scans are then processed in a pipeline of image tooling that classifies documents and generates text. This transcription is handed over to language software that recognizes war criminals, partisans, witnesses, and victims, and connects them to existing databanks.

In products such as Nayderlab and Pergamon Text we want to make every text available to a wide audience such as researchers and students, but also teachers, and the general public. Nayderlab and Pergamon Text are platforms that combine text and context. These tools bring together texts from many different authors and institutions across ages and add the ability to analyze them across time and space. We call these virtual collections.

Nayderlab has specific features to support these virtual collections. The platform aids the user in the search through time. For example through a method we call lexical query expansion. When the user enters a search term, the software knows that it needs to search for different words in different periods. The system helps the user in formulating a question by linking modern, early-modern, and medieval Dutch dictionaries in a network of words.

Finally we would like to show you Halicarnassus. Halicarnassus allows the user to come as close as possible to actual time travel. Sound Excited? Let me show you.
We extract information from sources and link occurrences in texts of people, places, and events, to known data. Such networks are called: knowledge graphs. And these also allow people to do research across time and space. Halicarnassus’ brings these two dimensions together.

As an example we have taken the travels of the Dutch seafarer Ahbel Tasman. In the first half of the 17th century he was send out twice in search of the mysterious land known as Terra Australis. From his logs we extracted his location to linked data, and mapped his first trip in time and space. If there was a medal for not finding Australia, Tasman would probably have won it. When looking at this, we cannot stop thinking, how can you miss this! At least he did manage to discover Tasmania, the island that still carries his name. And on his second trip he eventually discovered Australia and explored large sections of the continent’s northern coast.

This is just one visualisation but we are working hard to make it compatible with all the geographical data that is stored in our knowledge graphs. This allows you to visualize movement and follow the migration of people, objects, and ideas, through time and space.

So, these are just a few projects we are working on right now. There's a lot for us to do. But we're super excited about bringing the humanities and technology together.
We are convinced that the future of the humanities and social sciences is in solid data-driven science. Connected software systems are crucial for that. Now we're taking the next step: away from making the humanities more digital, and towards making the computational sciences more humanist!
